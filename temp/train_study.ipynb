{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a580d322-3f49-4d18-9c06-21f51af44fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import argparse\n",
    "import os\n",
    "import wandb\n",
    "from scipy.stats import pearsonr\n",
    "import audiofile\n",
    "from transformers import Wav2Vec2Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5989dbd3-672e-4825-a3d2-5ac7ec5bb4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 글로벌 변수 선언\n",
    "LANG = 'en'\n",
    "LABEL_TYPE1 = 'pron'\n",
    "LABEL_TYPE2 = 'prosody'\n",
    "DIR_LIST = 'data_list'\n",
    "DEVICE = 'cuda'\n",
    "DIR_MODEL = 'model'\n",
    "\n",
    "MODEL_TYPE = 'cnn+lstm'\n",
    "BASE_DIM = 1024\n",
    "#lstm, mlp\n",
    "MLP_HIDDEN = 512\n",
    "NUM_LAYERS = 4\n",
    "\n",
    "\n",
    "LR = 0.01\n",
    "EPOCHS = 200\n",
    "BATCH_SIZE = 256\n",
    "PATIENCE = 20\n",
    "NUM_WORKERS = 10\n",
    "AUDIO_LEN_MAX = 200000\n",
    "\n",
    "BASE_MODEL = None\n",
    "DIR_DATA = None\n",
    "DIR_RESUME = None\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bf1a82d-73be-4e32-8c5c-b1a3e81ab91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random seed 고정\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if DEVICE == 'cuda':\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3635ff18-ae6a-46c2-8989-5ad0cb9f8b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CNN_LSTM_RegressionModel(nn.Module):\n",
    "#     def __init__(self, input_dim=1024, hidden_dim=128, num_layers=2, dropout=0.3):\n",
    "#         super(CNN_LSTM_RegressionModel, self).__init__()\n",
    "#         self.cnn = nn.Sequential(\n",
    "#             nn.Conv2d(1, 32, kernel_size=(3, 3), padding=(1, 1)),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool2d(kernel_size=(2, 2)),\n",
    "#             nn.Conv2d(32, 64, kernel_size=(3, 3), padding=(1, 1)),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool2d(kernel_size=(2, 2))\n",
    "#         )\n",
    "#         # Adjust the LSTM input dimension based on the output of CNN\n",
    "#         cnn_output_dim = 64 * (input_dim // 16)\n",
    "#         self.lstm = nn.LSTM(cnn_output_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout)\n",
    "#         self.fc = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         batch_size, feature_dim = x.size()\n",
    "#         x = x.view(batch_size, 1, 32, -1)  # Adjust to [batch_size, 1, height, width] format\n",
    "#         x = self.cnn(x)\n",
    "#         x = x.view(batch_size, -1)\n",
    "#         x = x.unsqueeze(1)  # [batch_size, 1, cnn_output_dim]\n",
    "#         x, _ = self.lstm(x)\n",
    "#         x = x[:, -1, :]  # Get the last output of the LSTM\n",
    "#         x = self.fc(x)\n",
    "#         return x\n",
    "\n",
    "class CNN_LSTM_RegressionModel(nn.Module):\n",
    "    def __init__(self, input_dim=1024, hidden_dim=MLP_HIDDEN, num_layers=NUM_LAYERS, dropout=0.3):\n",
    "        super(CNN_LSTM_RegressionModel, self).__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=(3, 3), padding=(1, 1)),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2)),\n",
    "            nn.Conv2d(32, 64, kernel_size=(3, 3), padding=(1, 1)),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2))\n",
    "        )\n",
    "        # Adjust the LSTM input dimension based on the output of CNN\n",
    "        cnn_output_dim = 64 * (input_dim // 16)\n",
    "        self.lstm = nn.LSTM(cnn_output_dim, MLP_HIDDEN, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(MLP_HIDDEN, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, feature_dim = x.size()\n",
    "        x = x.view(batch_size, 1, 32, -1)  # Adjust to [batch_size, 1, height, width] format\n",
    "        x = self.cnn(x)\n",
    "        x = x.view(batch_size, -1)\n",
    "        x = x.unsqueeze(1)  # [batch_size, 1, cnn_output_dim]\n",
    "        x, _ = self.lstm(x)\n",
    "        x = x[:, -1, :]  # Get the last output of the LSTM\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d724e18f-23bf-47e5-ba07-90e0b502470c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_file(filename):\n",
    "    with open(filename) as f:\n",
    "        return f.readlines()\n",
    "\n",
    "def feat_extraction(data_type):\n",
    "    ''' wav2vec2 feature extraction part '''\n",
    "    fname_list = os.path.join(DIR_LIST, f'lang_{LANG}', f'{data_type}_list.txt')\n",
    "    fname_list = open_file(fname_list)\n",
    "    features = []\n",
    "    labels = []\n",
    "    for filename in fname_list:\n",
    "        filename = filename.strip()\n",
    "        audio_file = os.path.join(DIR_DATA, filename + '.wav')\n",
    "        audio_data, sampling_rate = audiofile.read(audio_file)\n",
    "        audio_data = audio_data[:AUDIO_LEN_MAX]\n",
    "        \n",
    "        # wav2vec2 model\n",
    "        model = Wav2Vec2ForCTC.from_pretrained(BASE_MODEL)\n",
    "        input_values = model.feature_extractor(audio_data, sampling_rate=sampling_rate, return_tensors='pt').input_values\n",
    "        with torch.no_grad():\n",
    "            features.append(model(input_values).logits.numpy().squeeze())\n",
    "\n",
    "        # load label\n",
    "        label_file = os.path.join(DIR_DATA, filename + f'_{LABEL_TYPE1}_{LABEL_TYPE2}.npy')\n",
    "        labels.append(np.load(label_file))\n",
    "\n",
    "    return np.array(features), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2223c2cb-e73d-488c-ae17-4a08e67037ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_or_extract_features(data_type):\n",
    "    \"\"\"Load features from file if they exist, otherwise extract them and save to file.\"\"\"\n",
    "    feature_dir = os.path.join(\"Datasets_full_list\", f\"lang_{LANG}\")\n",
    "    os.makedirs(feature_dir, exist_ok=True)\n",
    "    feature_file = os.path.join(feature_dir, f\"{LABEL_TYPE1}_{data_type}.npz\")\n",
    "\n",
    "    if os.path.exists(feature_file): # 파일이 존재하면 로드\n",
    "        print(f\"Loading features from {feature_file}\")\n",
    "        data = np.load(feature_file)\n",
    "        feat_X, feat_Y = data[\"X\"], data[\"Y\"]\n",
    "    else: # 존재하지 않으면 추출\n",
    "        print(f\"Extracting features and saving to {feature_file}\")\n",
    "        feat_X, feat_Y = feat_extraction(data_type)\n",
    "        np.savez(feature_file, X=feat_X, Y=feat_Y)\n",
    "\n",
    "    print(f\"wav2vec2 feature {data_type}, {feat_X.shape}, {feat_Y.shape}\")\n",
    "    return feat_X, feat_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe0183f8-24a3-4acd-95d2-2b0f4443cc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    trn_feat_x, trn_feat_y = load_or_extract_features('trn')  # feature extraction or loading for training data\n",
    "    val_feat_x, val_feat_y = load_or_extract_features('val')  # feature extraction or loading for validation data\n",
    "    test_feat_x, test_feat_y = load_or_extract_features('test')  # feature extraction or loading for test data\n",
    "\n",
    "    tr_dataset = TensorDataset(torch.tensor(trn_feat_x), torch.tensor(trn_feat_y))\n",
    "    val_dataset = TensorDataset(torch.tensor(val_feat_x), torch.tensor(val_feat_y))\n",
    "    test_dataset = TensorDataset(torch.tensor(test_feat_x), torch.tensor(test_feat_y))\n",
    "\n",
    "    train_dataloader = DataLoader(tr_dataset, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, pin_memory=True, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)\n",
    "    print(f\"wav2vec2 feature LOADED!!\") \n",
    "    return train_dataloader, val_dataloader, test_dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "721dd94a-e0d2-4e7e-894f-12ae5bf8ef03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_pearsonr(labels, preds):\n",
    "    if np.all(labels == labels[0]) or np.all(preds == preds[0]):\n",
    "        return 0  # If all values are the same, set PCC to 0\n",
    "    else:\n",
    "        return pearsonr(labels, preds)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed6d4f1a-dd9a-4b9a-a3e3-d9d18cadefee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcoldbrew\u001b[0m (\u001b[33mx_team\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/coldbrew/fluent/01.발음평가모델/1.모델소스코드/wandb/run-20240521_230108-m1en2bvt</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/x_team/model_study_pron%2Barticulation/runs/m1en2bvt' target=\"_blank\">earnest-durian-35</a></strong> to <a href='https://wandb.ai/x_team/model_study_pron%2Barticulation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/x_team/model_study_pron%2Barticulation' target=\"_blank\">https://wandb.ai/x_team/model_study_pron%2Barticulation</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/x_team/model_study_pron%2Barticulation/runs/m1en2bvt' target=\"_blank\">https://wandb.ai/x_team/model_study_pron%2Barticulation/runs/m1en2bvt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base wav2vec2 model: facebook/wav2vec2-large-robust-ft-libri-960h\n",
      "Loading features from Datasets_full_list/lang_en/pron_trn.npz\n",
      "wav2vec2 feature trn, (64748, 1024), (64748, 1)\n",
      "Loading features from Datasets_full_list/lang_en/pron_val.npz\n",
      "wav2vec2 feature val, (17958, 1024), (17958, 1)\n",
      "Loading features from Datasets_full_list/lang_en/pron_test.npz\n",
      "wav2vec2 feature test, (8813, 1024), (8813, 1)\n",
      "wav2vec2 feature LOADED!!\n",
      "Training a model from scratch\n",
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/coldbrew/anaconda3/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0,train loss\": 1.4039912303917021, eval_pcc: 0.2787077654781437, test_pcc: 0.27359563157537303\n",
      "epoch 1,train loss\": 0.7997416707838005, eval_pcc: 0.3787144069304195, test_pcc: 0.37076371866327396\n",
      "epoch 2,train loss\": 0.8002692102914742, eval_pcc: 0.4027585246019682, test_pcc: 0.39069212412895593\n",
      "epoch 3,train loss\": 0.6535092359003813, eval_pcc: 0.5764474338600057, test_pcc: 0.5687934499672395\n",
      "epoch 4,train loss\": 0.5471796608960675, eval_pcc: 0.6148732506293988, test_pcc: 0.6073116044850868\n",
      "epoch 5,train loss\": 0.5188709509231356, eval_pcc: 0.6424666944005856, test_pcc: 0.633396571167901\n",
      "epoch 6,train loss\": 0.4947819580202517, eval_pcc: 0.6633757717609917, test_pcc: 0.6569927682141887\n",
      "epoch 7,train loss\": 0.46986798241204425, eval_pcc: 0.6724403418016263, test_pcc: 0.6659701545968604\n",
      "epoch 8,train loss\": 0.4682197033652204, eval_pcc: 0.6727839023790146, test_pcc: 0.6692036038863377\n",
      "epoch 9,train loss\": 0.4634464856664183, eval_pcc: 0.678227590451191, test_pcc: 0.6740430671570368\n",
      "epoch 10,train loss\": 0.4588805828169872, eval_pcc: 0.6836111737507153, test_pcc: 0.6797127246264281\n",
      "epoch 11,train loss\": 0.4570512413507394, eval_pcc: 0.6802400117864706, test_pcc: 0.6741270782064943\n",
      "epoch 12,train loss\": 0.450040655644986, eval_pcc: 0.6872506825220079, test_pcc: 0.6805405244394684\n",
      "epoch 13,train loss\": 0.45048618929188244, eval_pcc: 0.6893603682174613, test_pcc: 0.6820976113346533\n",
      "epoch 14,train loss\": 0.4470408019341028, eval_pcc: 0.689822849060713, test_pcc: 0.6852246505264117\n",
      "epoch 15,train loss\": 0.44418295464025653, eval_pcc: 0.6864768264921309, test_pcc: 0.6817016511164524\n",
      "epoch 16,train loss\": 0.444230541999161, eval_pcc: 0.6883336722785984, test_pcc: 0.6815035074303832\n",
      "epoch 17,train loss\": 0.44297397666769067, eval_pcc: 0.6842923748331479, test_pcc: 0.6814639496284068\n",
      "epoch 18,train loss\": 0.4979490957005693, eval_pcc: 0.6879864534351825, test_pcc: 0.6821239971625804\n",
      "epoch 19,train loss\": 0.44620572802106384, eval_pcc: 0.6863052973522811, test_pcc: 0.6836324833309433\n",
      "epoch 20,train loss\": 0.44548686796968634, eval_pcc: 0.6832154829897646, test_pcc: 0.6777317807197439\n",
      "epoch 21,train loss\": 0.44115730330878095, eval_pcc: 0.686566134606388, test_pcc: 0.6812444602723975\n",
      "epoch 22,train loss\": 0.4421836164864627, eval_pcc: 0.6867834692959939, test_pcc: 0.6827796379576323\n",
      "epoch 23,train loss\": 0.4331488559839754, eval_pcc: 0.6846823247001219, test_pcc: 0.6807001008816297\n",
      "epoch 24,train loss\": 0.4388452305388545, eval_pcc: 0.6706975559595645, test_pcc: 0.6650056539404992\n",
      "epoch 25,train loss\": 0.4459182510498484, eval_pcc: 0.6919374690599908, test_pcc: 0.6854378731244147\n",
      "epoch 26,train loss\": 0.43740265242195886, eval_pcc: 0.6923873867792604, test_pcc: 0.6855018588647499\n",
      "epoch 27,train loss\": 0.43705654426996887, eval_pcc: 0.6854752227228774, test_pcc: 0.6781486426722683\n",
      "epoch 28,train loss\": 0.4446737080694658, eval_pcc: 0.6952721657583314, test_pcc: 0.6913323723934639\n",
      "epoch 29,train loss\": 0.43818303612852283, eval_pcc: 0.6931791931368937, test_pcc: 0.6869575991090607\n",
      "epoch 30,train loss\": 0.44434273926165735, eval_pcc: 0.6858959457198431, test_pcc: 0.68064681929879\n",
      "epoch 31,train loss\": 0.4465047646416977, eval_pcc: 0.682768121151484, test_pcc: 0.6774665012831789\n",
      "epoch 32,train loss\": 0.43884529450194165, eval_pcc: 0.6890671869437892, test_pcc: 0.6864541760803148\n",
      "epoch 33,train loss\": 0.44196452029608924, eval_pcc: 0.6809830285925759, test_pcc: 0.6741559506560701\n",
      "epoch 34,train loss\": 0.44070217979284143, eval_pcc: 0.6935836317086197, test_pcc: 0.6892941515795522\n",
      "epoch 35,train loss\": 0.44429725985753205, eval_pcc: 0.6869339882627619, test_pcc: 0.6826607856304698\n",
      "epoch 36,train loss\": 0.4380012252349627, eval_pcc: 0.6919565268293396, test_pcc: 0.6873841698765606\n",
      "epoch 37,train loss\": 0.4439790187617064, eval_pcc: 0.6868434292239232, test_pcc: 0.6835458195857701\n",
      "epoch 38,train loss\": 0.438783494733539, eval_pcc: 0.689693468130991, test_pcc: 0.6835550200050868\n",
      "epoch 39,train loss\": 0.44497879303019977, eval_pcc: 0.6851646962783492, test_pcc: 0.678471253616884\n",
      "epoch 40,train loss\": 0.4417178085434578, eval_pcc: 0.6885805913060596, test_pcc: 0.6857106077732256\n",
      "epoch 41,train loss\": 0.4369734532748286, eval_pcc: 0.693266143301694, test_pcc: 0.6890344075844812\n",
      "epoch 42,train loss\": 0.43917009611374774, eval_pcc: 0.6879828863166935, test_pcc: 0.6841009152498281\n",
      "epoch 43,train loss\": 0.44260508952875854, eval_pcc: 0.6913680172397723, test_pcc: 0.6858359525489882\n",
      "epoch 44,train loss\": 0.44556330091397284, eval_pcc: 0.6933639521811511, test_pcc: 0.6868601683427563\n",
      "epoch 45,train loss\": 0.44174771641082916, eval_pcc: 0.6914435178778924, test_pcc: 0.6836748147723545\n",
      "epoch 46,train loss\": 0.4438569617601251, eval_pcc: 0.6887076177382456, test_pcc: 0.6860807670859944\n",
      "epoch 47,train loss\": 0.4406018870856922, eval_pcc: 0.689793240137977, test_pcc: 0.6859563736397352\n",
      "epoch 48,train loss\": 0.4424322178005701, eval_pcc: 0.6814787972356104, test_pcc: 0.6809646287719435\n",
      "epoch 49,train loss\": 0.4429016502007194, eval_pcc: 0.6951716377364144, test_pcc: 0.691698823928508\n",
      "Early stopping triggered.\n",
      "Final Test PCC: 0.6913323723934639\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>eval_pcc</td><td>▁▃▃▆▇▇██████████████████████████████████</td></tr><tr><td>final_test_pcc</td><td>▁</td></tr><tr><td>learning_rate</td><td>▁▁▁▂▂▃▃▄▅▆▆▇▇███████████████████████████</td></tr><tr><td>test_pcc</td><td>▁▃▃▆▇▇██████████████████████████████████</td></tr><tr><td>train_loss</td><td>█▄▄▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▇█▇▃▃▂▂▂▂▂▂▂▁▁▂▁▂▁▁▁▁▂▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>49</td></tr><tr><td>eval_pcc</td><td>0.69517</td></tr><tr><td>final_test_pcc</td><td>0.69133</td></tr><tr><td>learning_rate</td><td>0.00933</td></tr><tr><td>test_pcc</td><td>0.6917</td></tr><tr><td>train_loss</td><td>0.4429</td></tr><tr><td>val_loss</td><td>0.44944</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">earnest-durian-35</strong> at: <a href='https://wandb.ai/x_team/model_study_pron%2Barticulation/runs/m1en2bvt' target=\"_blank\">https://wandb.ai/x_team/model_study_pron%2Barticulation/runs/m1en2bvt</a><br/> View project at: <a href='https://wandb.ai/x_team/model_study_pron%2Barticulation' target=\"_blank\">https://wandb.ai/x_team/model_study_pron%2Barticulation</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240521_230108-m1en2bvt/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def train():\n",
    "    global BASE_MODEL\n",
    "    # Initialize wandb\n",
    "    config = {\n",
    "        \"lang\": LANG,\n",
    "        \"label_type1\": LABEL_TYPE1,\n",
    "        \"label_type2\": LABEL_TYPE2,\n",
    "        \"dir_list\": DIR_LIST,\n",
    "        \"audio_len_max\": AUDIO_LEN_MAX,\n",
    "        \"device\": DEVICE,\n",
    "        \"num_workers\": NUM_WORKERS,\n",
    "        \"base_model\": BASE_MODEL,\n",
    "        \"dir_model\": DIR_MODEL,\n",
    "        \"dir_data\": DIR_DATA,\n",
    "        \"dir_resume\": DIR_RESUME,\n",
    "        \"base_dim\": BASE_DIM,\n",
    "        \"mlp_hidden\": MLP_HIDDEN,\n",
    "        \"lr\": LR,\n",
    "        \"epochs\": EPOCHS,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"patience\": PATIENCE,\n",
    "        \"model_type\": MODEL_TYPE\n",
    "    }\n",
    "    wandb.init(project=\"model_study_pron+articulation\", config=config)\n",
    "    dir_save_model = f'{DIR_MODEL}/lang_{LANG}'\n",
    "    os.makedirs(dir_save_model, exist_ok=True)\n",
    "\n",
    "    if LANG == 'en':\n",
    "        base_model = 'facebook/wav2vec2-large-robust-ft-libri-960h'\n",
    "    print(f'base wav2vec2 model: {base_model}')\n",
    "    \n",
    "    train_dataloader, val_dataloader, test_dataloader = load_data()\n",
    "\n",
    "    if MODEL_TYPE =='transformer':\n",
    "    # Model hyperparameters\n",
    "        hidden_dim = 8   # Transformer hidden dimension\n",
    "        num_layers = 2     # Number of Transformer layers\n",
    "        num_heads = 1      # Number of attention heads\n",
    "        output_dim = 1     # Regression output dimension (1 for a single value)\n",
    "    # elif MODEL_TYPE =='cnn+lstm':\n",
    "    #     hidden_dim = 128   # LSTM hidden dimension\n",
    "    #     num_layers = 2     # Number of LSTM layers\n",
    "    #     dropout = 0.3      # Dropout rate\n",
    "\n",
    "    net = CNN_LSTM_RegressionModel(input_dim=BASE_DIM).to(DEVICE)\n",
    "\n",
    "    if DIR_RESUME is not None:\n",
    "        dir_resume_model = os.path.join(DIR_RESUME, f'lang_{LANG}', f'{LABEL_TYPE1}_{LABEL_TYPE2}_checkpoint.pt')\n",
    "        net.load_state_dict(torch.load(dir_resume_model, map_location=DEVICE))\n",
    "        print(f'Training a model from {dir_resume_model}')\n",
    "    else:\n",
    "        print(f'Training a model from scratch')\n",
    "\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=LR)  # training optimizer\n",
    "    loss_func = torch.nn.MSELoss()  # MSE loss for regression task\n",
    "\n",
    "    # 학습률 스케줄러 추가\n",
    "    steps_per_epoch = len(train_dataloader)\n",
    "    total_steps = EPOCHS * steps_per_epoch\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer,\n",
    "        max_lr=LR,\n",
    "        total_steps=total_steps,\n",
    "        pct_start=0.1,  # 웜업 비율 (첫 10% 단계에서 학습률 증가)\n",
    "        anneal_strategy='cos',  # 코사인 에닐링\n",
    "        cycle_momentum=False  # Adam에서는 False로 설정\n",
    "    )\n",
    "    \n",
    "    eval_best_pcc = -9\n",
    "    early_stop_counter = 0\n",
    "    stop_flag = False\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    for epoch in range(EPOCHS):\n",
    "        net.train()\n",
    "        train_loss = 0\n",
    "        for train_data in train_dataloader:\n",
    "            feat_x, feat_y = train_data\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            prediction = net(feat_x.to(DEVICE))\n",
    "            loss = loss_func(prediction, feat_y.to(DEVICE))\n",
    "            \n",
    "            # Gradient Clipping\n",
    "            torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm=1.0)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()  # 스케줄러 단계 업데이트\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss /= len(train_dataloader)\n",
    "        current_lr = optimizer.param_groups[0]['lr']  # 현재 학습률 가져오기\n",
    "        wandb.log({\"epoch\": epoch, \"train_loss\": train_loss, \"learning_rate\": current_lr})  # 학습률을 로그에 추가\n",
    "\n",
    "        # Validation\n",
    "        net.eval()\n",
    "        val_loss = 0\n",
    "        val_labels = []\n",
    "        val_preds = []\n",
    "        with torch.no_grad():\n",
    "            for val_data in val_dataloader:\n",
    "                feat_x, feat_y = val_data\n",
    "\n",
    "                val_labels.extend(feat_y.tolist())\n",
    "                prediction = net(feat_x.to(DEVICE))\n",
    "                val_preds.extend(prediction.cpu().tolist())\n",
    "\n",
    "                loss = loss_func(prediction, feat_y.to(DEVICE))\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        val_loss /= len(val_dataloader)\n",
    "        val_labels = np.array(val_labels).squeeze()\n",
    "        val_preds = np.clip(np.array(val_preds).squeeze(), 0, 5)\n",
    "\n",
    "        eval_pcc = calculate_pearsonr(val_labels, val_preds)\n",
    "        wandb.log({\"epoch\": epoch, \"val_loss\": val_loss})\n",
    "\n",
    "        # Testing\n",
    "        net.eval()\n",
    "        test_labels = []\n",
    "        test_preds = []\n",
    "        with torch.no_grad():\n",
    "            for test_data in test_dataloader:\n",
    "                feat_x, feat_y = test_data\n",
    "                \n",
    "                test_labels.extend(feat_y.tolist())\n",
    "                prediction = net(feat_x.to(DEVICE))\n",
    "                test_preds.extend(prediction.cpu().tolist())\n",
    "\n",
    "        test_labels = np.array(test_labels).squeeze()\n",
    "        test_preds = np.clip(np.array(test_preds).squeeze(), 0, 5)\n",
    "\n",
    "        test_pcc = calculate_pearsonr(test_labels, test_preds)\n",
    "        \n",
    "        print(f'epoch {epoch},train loss\": {train_loss}, eval_pcc: {eval_pcc}, test_pcc: {test_pcc}')\n",
    "        wandb.log({\"epoch\": epoch,\"train loss\": {train_loss}, \"eval_pcc\": eval_pcc, \"test_pcc\": test_pcc})\n",
    "\n",
    "        # Early stopping\n",
    "        if eval_pcc > eval_best_pcc and not stop_flag:\n",
    "            eval_best_pcc = eval_pcc\n",
    "            test_best_pcc = test_pcc\n",
    "            early_stop_counter = 0\n",
    "            torch.save(net.state_dict(), os.path.join(dir_save_model, f'{LABEL_TYPE1}_{LABEL_TYPE2}_{MODEL_TYPE}_checkpoint.pt'))\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "\n",
    "        if early_stop_counter > PATIENCE and not stop_flag:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "    print(f'Final Test PCC: {test_best_pcc}')\n",
    "    wandb.log({\"final_test_pcc\": test_best_pcc})\n",
    "    wandb.finish()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2fb285f2-89a6-4935-ac22-cfeb14d15e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# *transformer train command* \n",
    "#python train_transformer.py --lang='en' --label_type1='pron' --label_type2='articulation'  --dir_list='/home/coldbrew/fluent/01.발음평가모델/1.모델소스코드/datasets_full_list' --epochs=200 --patience=20 --batch_size=256 --dir_model='model_transformer' --model_type='transformer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80e68ca-ee10-4edc-9fa7-088314592300",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d217bf-1d51-40b1-8f08-8a170ce44ec8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abcec7cb-e17c-44ff-bb56-311a6c016713",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3540eb3-b417-4d62-a129-5a5d559a8904",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ce3146-b4a4-4840-85c2-68ed6110b609",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f48af1-0911-4c30-b6fc-622187eb9757",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fluent-gpu",
   "language": "python",
   "name": "fluent-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
